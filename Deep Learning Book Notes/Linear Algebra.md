## 2.1 Scalars, Vectors, Matrices and Tensors

### Scalars:
A scalar is a single number.

### Vectors
A vector is an algebric structure, mostly described as a list of scalars.  
Example of vectors:
* In Phisycs: An arrow pointing to a direction

### Matrices
2D vector, containing scalars (basically a vector of vectors).  
In geometry a matrix represents a transformation of a vector space. This matrix multiplied with a vector gives the coresponding vector in the transformed space.

### Tensor
an N-Dimensional array of scalars

## 2.2 Multiplying Matrices and Vectors
* Normal Multiplication of matrices: $$ C_{i,j} = \sum_{k} A_{i,k} * B_{k,j} $$
* Hadamard Product (Element wise): $$ A \odot B $$
* Dot Product (Vectors) - result is a scalar, basically normal multiplication of matrices: $$ x^\mathsf{T}y $$
* Distributivity: $$ A(B + C) = AB + AC $$
* Asociativity: $$ A(BC) = (AB)C $$
* Transpose: $$ (AB)^\mathsf{T} = B^\mathsf{T}A^\mathsf{T} $$

## 2.3 Identity and Inverse Matrices
* Identity: $$ AI_n = I_nA = A $$
* Inverse: $$ A^{-1}A = AA^{-1} = I_n $$

## 2.4 Linear Dependance and Span

### Span
The list of vectors that can be computed as a linear combination of any elements of a set of vectors.  
In geometry it can be seen as the total space generated by a set of vectors.

### Linear Dependence
A set of vectors is linear dependent if at least one of those vectors can be written as a linear combination of the others: $$ v^{(i)} = \sum_{k\neq i} a_kv^{(k)} $$  
Any set that doesn't contain any such vector is called linear independent

## 2.5 Norms
Mainly described as the distance between the origin and the point.  
Must respect 3 properties:  
* $$ f(x) = 0 => x = 0 $$
* $$ f(x + y) \leq f(x) + f(y) \text{  (the triangle inequality)} $$
* $$ \forall \alpha \in \mathbb{R} , f(\alpha x) = |\alpha|f(x) $$

Common Norms:
$$ L^n = (\sum_{k} |x_k|^n)^{\frac{1}{n}}$$
* $L^2$ also known as Euclidean norm (distance): $$ L^2 = \sqrt{\sum_{k} |x_k|^2}$$
  * Most common, mostly written as $||x||$
  * Squared $L_2$ is more used because it is easier to compute: $x^tx$
* $L^1$ also known in 2D planes as Manhattan distance: $$ L^1 = \sum_{k} |x_k| $$
  * Commonely used when the difference between zero and nonzero elements is important
* $L^{\infty}$ pretty much taking the greates element, also known as max norm
* Frobenius Norm, basically $L^2$ but for matrices: $$||A|| = \sqrt{\sum_{i,j} A_{i,j}^2}$$

## 2.6 Special Kinds of Matrices and Vectors
* Diagonal:
  * Has nonzero entries only on the main diagonal
  * Easy to multiply
  * Easy invertible
* Symetric: 
  * $A = A^T$
* Unit Vector (Normal vector):
  * $ || x || = 1 $
* Orthogonal Matrix:
  * Orthonormal vectors, 2 unit vectors that are perpendicular on eachother: $x^Ty = 0$
  * Squared matrix that has all rows and columns mutually orthonormal

## 2.7 Eigendecomposition
A way to decompose a matrix into eigenvectors and eigenvalues.  
If we see a matrix as a transformation, eigenvectors are those vectors that are only scaled after the transformation is applied: $Av = \lambda v$.  
Any scaled eigenvector is an eigenvector as well, so we only care about unit eigenvectors.  
Lambda is that eigenvalue, is the value with which the vector is scaled.  
If A has n linearly independent eigenvectors, we can concatenate them one per column, so that we have an eigendecomposition: $A = V{diag(\lambda)}V^{-1}$ 
A matrix can be definite if it has real eigenvalues:
* Positive definite <= all positive
* Positive semidefinite <= all positive or zero
* Negative semidefinite <= all negative or zero
* Negative definite <= all negative

## 2.8 Singular Value Decomposition (SVD)
It is another way to factorize a matrix, but more generally applicable.  
We want to find a set of orthonormal vectors that when transformed by A will remain orthonormal:  
$Av_i = \sigma_i u_i$ -> $A[v_1 .. v_n] = [\sigma_1 u_1 .. \sigma_n u_n]$ -> $A[v_1 .. v_n] = [u_1 .. u_n]{diag(\sigma)}$ -> $AV = U\Sigma$ -> $A = U\Sigma V^{-1}$  
* $\Sigma$ -> diagonal matrix containing the singular values of A
* U -> orthogonal matrix containing left-singular vectors of A ( the eigenvectors of $AA^T$ )
* V -> orthogonal matrix containing right-singular vectors of A ( the eigenvectors of $A^TA$ )
